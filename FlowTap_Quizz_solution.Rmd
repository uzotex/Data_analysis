---
title: "FlowTaP Data Science Test"
author: "Uzo J Iwuagwu"
date: "May 24, 2017"
output:
  pdf_document: default
  html_document: default
---

## 1. Engineering a Data Set
Create a list of numbers with the following properties: 
1) Minimum 100 distinct values, 
2) the Mean of all values is 1000 (+/- 0.5), 
3) the standard Deviation is 10 (+/- 0.1)

```{r  Engineering a Data Set}
set.seed(1)
value = rnorm(n = 500, mean = 1000, sd = 10)
value
round(mean(value), 1)
round(sd(value), 1)
```

## 2. Simulation

Imagine there is a country in which couples only want boys.
Couples continue to have children until they have their first boy. If they get a boy, they stop getting children.What is the long-term ratio of boys to girls in the country?

```{r Simulation}
p<-0
for (i in 1:10000){
  a<-0
  while(a != 1){   #Stops when having girls
    a<-as.numeric(rbinom(1, 1, 0.5))   #Simulation of a new birth with probability 0.5
    p=p+1   #Number of births
  }
}
(p-10000)/10000   #Ratio
```
 
## 3. Analyzing a Data Set

Explore the data and visualize and explain two interesting findings of your choice. Ideally, use R and ggplot2 for this task

```{r ggplot2_packages}
#install.packages("ggplot2")
#install.packages("vcd")
#install.packages("ggthemes")
#install.packages("ResourceSelection")
library(vcd)
library("ggplot2")
library(plyr)

getwd()
setwd("C:/Users/PCL/Documents/FlowTap/FlowTap_Quizz")
```

Clean and read the Adult data set

```{r Analyzing Data Set}
adult_data =read.table("C:/Users/PCL/Documents/FlowTap/FlowTap_Quizz/adult.data.txt", header = TRUE, sep = ",", col.names = c("Age", "workclass", "fnlwgt", "Education", "Educationnum", "Maritalstatus", "Occupation", "Relationship", "race", "Sex", "Capitalgain", "Capitalloss", "hoursperweek", "NativeCountry", "Class"), fill = FALSE, na.strings = "?", strip.white = TRUE)
adult_data

```

Check for missing value:
```{r Missing Values}
   any(is.na(adult_data))
```
Remove Missing Values
```{r Remove missing value}
    adult_data <- na.omit(adult_data)
      adult_data
```

Clean and Read Test data set
```{r Analyzing Test Data Set}
adult_test =read.table("C:/Users/PCL/Documents/FlowTap/FlowTap_Quizz/adult.test.txt", header = TRUE, sep = ",", col.names = c("Age", "workclass", "fnlwgt", "Education", "Education-num", "Marital-status", "Occupation", "Relationship", "race", "Sex", "Capital-gain", "Capital-loss", "hoursperweek", "Native Country", "Class"), fill = FALSE, strip.white = TRUE)
adult_test
```

# visualization

The distribution of the  class <=50k and >50k

Summary Statistics
```{r Summary Statistics}
summary(adult_data$Class)
```

```{r barplot}

ggplot(data = adult_data, 
       mapping = aes(x = adult_data$Class, fill = adult_data$Class)) + 
  geom_bar(mapping = aes(y = (..count..)/sum(..count..))) +
  geom_text(mapping = aes(label = scales::percent((..count..)/sum(..count..)),
                      y = (..count..)/sum(..count..) ), 
            stat = "count",vjust = -.1) + labs(x = "Class",   y = "", fill = "Class")

```

Findings: We can see that 75.1% are below or equal the class 50k income earners while 24.9% are above the class 50k of income earners.

# Visualization 2

```{r Age Summary}
 summary(adult_data$Age)
 IQR(adult_data$Age)
```
Finding: The median age is 37 years and the mean age is 38 years. The quartile tells us that that most working are between 28 and 47 years old. There are some outliers, such as individuals being between 75 and 90 years old. To visualize the summary statistic we also show a box plot of the variable "Age":

```{r Age Boxplot}
 ggplot(mapping = aes(x = factor(0), y = Age), data = adult_data) + geom_boxplot() +
  stat_summary(fun.y = mean, geom = 'point', shape = 19, color = "red",cex = 2) +
  coord_cartesian(ylim = c(10, 100)) +
  scale_y_continuous(breaks = seq(10, 100, 5)) +
  ylab("Age") +
  xlab("") +  
  ggtitle("Box Plot of Age") +
  scale_x_discrete(breaks = NULL)
```

Finding from the Histogram below shows that 20 to 50 are the working age group.

```{r Age Histogram}
qplot(x = adult_data$Age, data = adult_data, binwidth = 5, color = I('black'), fill = I('#F29025'),xlab = "Age", ylab = "Count", main = "Histogram of Age") + scale_x_continuous(breaks = seq(0, 95, 5)) +  scale_y_continuous(breaks = seq(0, 4500, 500))
```

## linear model to predict wether a person makes over 50K a year

```{r Check and Install all packages}
if (!require(caTools)){
    install.packages('caTools', repos='http://cran.us.r-project.org')
    }

if (!require(ROCR)){
    install.packages('ROCR', repos='http://cran.us.r-project.org')
    }


if (!require(rpart)){
    install.packages('rpart', repos='http://cran.us.r-project.org')
    }


if (!require(randomForest)){
    install.packages('randomForest', repos='http://cran.us.r-project.org')
    }

if (!require(caret)){
    install.packages('caret', repos='http://cran.us.r-project.org')
    }


if (!require(e1071)){
    install.packages('e1071', repos='http://cran.us.r-project.org')
    }


if (!require(rpart.plot)){
    install.packages('rpart.plot', repos='http://cran.us.r-project.org')
    }


if (!require(ggplot2)){
    install.packages('ggplot2', repos='http://cran.us.r-project.org')
    }

```

```{r}
#library(ggplot2)
#library(plyr)
#library(vcd)
library(ggthemes)
library(caret)
library(ResourceSelection)
library(randomForest)
library(e1071)
library(nnet)
```

Since we are predicting the values of the variable "class". Therefore class" is our response variable, or dependent variable. It assumes only two values - earn less than 50K and more than 50K a year.
Using Y as the class in binary formatof 1 and 0.
if class >50k then Y = 1 else Y = 0

Therefore we are using logistic regression which is a type of linear model.

##Fitting a logistic regression model with the function glm

In fitting a logistic regression model, in "glm" we set "family" to "binomial" and "link" (link function) to "logit".

```{r names}
names(adult_data)
```

```{r Logistic Regression}

covariates = paste("Age", "workclass", "Education", "Educationnum",
                    "Maritalstatus", "Occupation", "Relationship",
                    "race", "Sex", "Capitalgain", "Capitalloss","hoursperweek", "NativeCountry",
                    sep = "+")

form <- as.formula(paste("Class~", covariates))

start_time = proc.time()
glm.model = glm(formula = form,
                 data = adult_data, 
                 family = binomial(link = "logit"),
                 x = TRUE)
# The option "x=TRUE" returns the design matrix
time.logistic <- proc.time() - start_time
time.logistic
```

## Check for collinear predictator variables

```{r Summary of Model}
summary(glm.model)$coefficients[, 1:2]
```
In the coefficients table the coefficient for the variable "Educationnum" is NA, i.e. with a missing value,this is an indication that the covariate "educationnum" is collinear with some other predictors. Therefore, we have to exclude it from the list of predictor variables and fit the model again. 

```{r test whether the covariate "educationnum" is collinear with some of the other predictors} 
findLinearCombos(glm.model$x)
```

```{r Find and Remove Collinear variables }
findLinearCombos(glm.model$x)$remove
colnames(glm.model$x)[findLinearCombos(glm.model$x)$remove]
```

As we can see from the code output above, R found linear dependencies between the covariates and recommends to 
remove column 24 from the design matrix, also we identify which predictor corresponds to column 24.

```{r Unique Combination}
unique.combinations <- unique(adult_data[ ,c("Education", "Educationnum")])

unique.combinations[order(unique.combinations$Educationnum), ]
```

From the content of "Educationnum" and "Education" we can also see that the two variables are linearly dependent, 
i.e. collinear.Therefore we remove the covariate "educationnum" and fit a new model - "glm.model2

```{r Model2 }
covariates_new = paste("Age", "workclass", "Education",
                    "Maritalstatus", "Occupation", "Relationship",
                    "race", "Sex", "Capitalgain", "Capitalloss","hoursperweek", "NativeCountry",
                    sep = "+")

form_new <- as.formula(paste("Class ~", covariates_new))

start_time <- proc.time()
glm.model2 <- glm(formula = form_new,
                     data = adult_data, 
                     family = binomial(link = "logit"),
                     x = TRUE,
                     y = TRUE)
time.logistic <- proc.time() - start_time
time.logistic

```

Check for collinear 
```{r}
findLinearCombos(glm.model2$x)
```

## Assessing the goodness of fit of the model
Since we have ungrouped data - we carry outlikelihood test with Hosmer-Lemeshow test

```{r Extract the fitted probabilities }

predicted.probs <- predict(glm.model2, type = "response")

head(predicted.probs) # returns probabilities
```

value of 1 is equivalent to an yearly income class of more than 50K, and a value of 0 means an income of less than 50K.

We take the vector of observed responses (which is a factor variable with two levels - " >50K" and " <=50K") and create a binary vector:

```{r Create a binary Vector}
observed.values <- ifelse(adult_data$Class  == " >50K", 1, 0)

```

Next we generate the vector of predicted probabilities - "predicted.probs", and then we use it to create the 
binary vector "predicted.response":

```{r creating binary Vector cont.. }
predicted.probs <- predict(glm.model2, type = "response")

predicted.response <- ifelse(predicted.probs > 0.5, 1, 0)

head(predicted.response, 20)
```


```{r oobserved values}
head(observed.values, 20)
```

First we test the accuracy of the fitted logistic model on the training dataset, i.e. we calculate what is the 
average percentage of correctly predicted response values:

```{r Predicted response}
mean(observed.values == predicted.response)
```
There is a 79.47% match between observed and predicted values of the dependent variable.

## We run the Hosmer-Lemeshowtest with different number of groups. We take g=10,20,50,100,200,300 and 400
```{r hoslem}
hoslem.test(observed.values, predicted.response, g=10)
hoslem.test(observed.values, predicted.response, g=20)
hoslem.test(observed.values, predicted.response, g=50)
hoslem.test(observed.values, predicted.response, g=100)
hoslem.test(observed.values, predicted.response, g=200)
hoslem.test(observed.values, predicted.response, g=300)
hoslem.test(observed.values, predicted.response, g=400)
```
## Significance of the explanatory variables in the model

We will carry out Overall significance of the categorical covariates in the model, first we will  perform likelihood ratio tests with the R function "anova()". When we run "anova(glm.model2, test="LRT")", the function sequentially compares nested models with increasing complexity against the full model by adding one predictor at a time

```{r Significance}
anova(glm.model2, test = "LRT")
```

As we can see from the results above, all explanatory variables are significant and according to the likelihood ratio tests we should keep all of the considered predictors in the model.



















